* Config/Preamble :noexport:

** Header

#+TITLE: Optimized GPU-Based Matrix Inversion
#+SUBTITLE: Through The Use of Thread-Data Remapping
#+AUTHOR: Samuel J. Monson
#+EMAIL: monsonsamuel@seattleu.edu
#+DATE: \today
#+LATEX_CLASS_OPTIONS: [letterpaper,8pt,hidelinks,twocolumn]
#+OPTIONS: toc:nil

** Emacs Config

#+startup: showeverything

#+BEGIN_SRC emacs-lisp :exports none :eval always
  (make-variable-buffer-local 'org-latex-title-command)
  ;; Use minted for code highlighting
  (setq org-latex-src-block-backend 'minted)
  ;; Don't add <center> tags to images I like to do that myself
  (setq org-latex-images-centered nil)
  ;; export snippet translations
  (add-to-list 'org-export-snippet-translation-alist
             '("l" . "latex"))
#+end_src

#+CITE_EXPORT: biblatex ieee
#+BIBLIOGRAPHY: sources.bib

** LaTeX Config

*** Use minted instead of verbatim

#+LATEX_HEADER: \usepackage{minted}

*** Spacing

#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing

*** Margins

#+LATEX_HEADER: \usepackage[total={7in,9in}]{geometry}
#+LATEX_HEADER: \setlength{\columnsep}{0.375in}

*** Numbering

#+LATEX_HEADER: \numberwithin{equation}{section} % Number equations by section

*** Reduce Hyphenation

#+LATEX_HEADER: \hyphenpenalty=5000
#+LATEX_HEADER: \tolerance=700

*** Setup Indentation

#+LATEX_HEADER: \usepackage[indent=2.5em]{parskip}

*** Set Font

Packages
#+LATEX_HEADER: \usepackage{titling} % For title
#+LATEX_HEADER: \usepackage{titlesec} % For section headings
#+LATEX_HEADER: \usepackage{unicode-math} % For font loading

Define fonts
#+LATEX_HEADER: \newfontfamily\headingfont{Libre Baskerville}
#+LATEX_HEADER: \setmainfont{DejaVuSerif}
#+LATEX_HEADER: \setmathfont{TeX Gyre DejaVu Math}
#+LATEX_HEADER: \setmathfont{Fira Math}[range={\infty}] % Steal some symbols
#+LATEX_HEADER: \AtBeginDocument{\renewcommand{\setminus}{\mathbin{\backslash}}} % Replace setminus with nice backslash

Set fonts
#+LATEX_HEADER: \titleformat*{\section}{\large\bfseries\headingfont}
#+LATEX_HEADER: \titleformat*{\subsection}{\normalsize\bfseries\headingfont}
#+LATEX_HEADER: \titleformat*{\subsubsection}{\normalsize\headingfont}
#+LATEX_HEADER: \renewcommand{\maketitlehooka}{\headingfont}

*** Define abs

#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \DeclarePairedDelimiter\abs{\lvert}{\rvert} % ABS: abs{}

*** Environments

Angled Small Vector
#+LATEX_HEADER: \newenvironment{asvector}{\left\langle\begin{smallmatrix}}{\end{smallmatrix}\right\rangle}

Angled Vector
#+LATEX_HEADER: \newenvironment{avector}{\left\langle\begin{matrix}}{\end{matrix}\right\rangle}

Tight Align
#+LATEX_HEADER: \newenvironment{talign}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
#+LATEX_HEADER: \newenvironment{talign*}{\[\begin{aligned}}{\end{aligned}\]}

Separated Matrices
#+LATEX_HEADER: \usepackage{nicematrix}

Fancy fractions
#+LATEX_HEADER: \usepackage{xfrac}

**** Theorems

#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \newtheoremstyle{indentbf}{.5\topsep}{.5em}{\addtolength{\leftskip}{2.5em}}{-1.5em}{\bfseries\headingfont}{}{\newline}{}
#+LATEX_HEADER: \newtheoremstyle{bf}{.5\topsep}{.5em}{}{}{\bfseries\headingfont}{}{.5em}{}

Theorem
#+LATEX_HEADER: \theoremstyle{bf}
#+LATEX_HEADER: \newtheorem{thm}{Theorem}[section]

Definition
#+LATEX_HEADER: \theoremstyle{indentbf}
#+LATEX_HEADER: \newtheorem{defn}{Definition}[section]

**** Algorithm

#+LATEX_HEADER: \usepackage[ruled,linesnumbered,commentsnumbered]{algorithm2e}

Allows for placing floats at top or bottom of twocolumn page
#+LATEX_HEADER: \usepackage{stfloats}

*** Citations

Show back-references to in-text citations
#+LATEX_HEADER: \usepackage[backref=true]{biblatex}
Change color of citations
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \hypersetup{colorlinks=true,allcolors=black,citecolor=teal,linkcolor=darkgray}
Make in-text citations smaller
#+LATEX_HEADER_EXTRA: \renewcommand*{\citesetup}{\biburlsetup\small\frenchspacing}

* TODO Abstract
:PROPERTIES:
    :UNNUMBERED: t
:END:

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

* Introduction

@@latex:{\Large M}@@atrix inversion is a fundamental component of Linear Algebra that has wide practical application in nearly all math-adjacent fields. Most notably, it has proven to be the best algebraic method for solving linear equations on a computer [cite:@press:2007]. This has made matrix inversion a critical component of everything from Computer Graphics to Machine Learning [cite:@anton:2014].

However, a significant drawback of matrix inversion lies in its computationally intensive nature. Traditional methods such as Gauss-Jordan [cite:@steven:1987] and LU-Decomposition [cite:@press:2007] possess a multiplicative asymptotic time complexity of $O(n^3)$ for any given $n \times n$ matrix. In 1969 Strassen [cite:@strassen:1969] introduced a new method that brought the complexity down to $O(n^{2.808})$. Following this discovery, various groups competed to improve on Strassen's results culminating in the Coppersmith and Winograd method [cite:@coppersmith:1981] which achieved a time complexity of $O(n^{2.376})$. Theoretically, the best sequential cost achievable for matrix inversion is $O(n^2)$ [cite:@cohn:2005]; however, no general algorithm has been found that achieves this cost.

While no general $O(n^2)$ inversion method has been found, there exist various methods that optimize for specific matrix types. Triangular matrices can be inverted using Gaussian Elimination in $O(n^2)$ time [cite:@press:2007]. The inverse of an orthogonal matrix is its transpose [cite:@anton:2014], thus the time complexity to invert an orthogonal matrix corresponds to the number of swap operations required to flip rows and columns, specifically $\frac{n^2}{2} - n$. Alternatively, if the matrix is simply read from memory in the new order, the time complexity is O(1). Cholesky Decomposition [cite:@press:2007] improves on LU-Decomposition for symmetric, positive-definite matrices and has a time complexity of $O\left(\frac{3}{4}n^3 + \frac{3}{2}n^2\right)$. QR-decomposition [cite:@press:2007] is slower then LU in most cases excluding where the inverses of multiple similar matrices are needed. For example, when we have a series of matrices where $A_{i+1} = A_i + B$ intermediate steps of the QR process of $A_i$ can be reused for $A_{i+1}$.

The Graphics Processing Unit (GPU) is a specialized co-processor originally created for the task of translating data representations into computer graphics. As modern display technology consists of a grid of independent points, GPUs have evolved to excel at embarrassingly parallel workloads. Due to this specialization, GPUs have found significant success outside of their originally designed function, particularly in scientific fields where extensive independent computation is crucial [cite:@nguyen:2007]. Since a grid of points is essentially a physical example of a matrix, GPUs are especially well-suited for operations involving matrices.

Due to the prolific use of matrix inversion in computing, an algorithm that improves on the time-complexity of existing inverse methods is of high demand. Thus, the goal of this paper is to outline a matrix inversion implementation that outperforms existing implementations in some or all scenarios. To achieve this goal, we built upon existing methods of paralleling matrix inversion and introduced a novel approach utilizing Thread-Data Remapping.

* Related Work

** Strassen Matrix Inversion
:PROPERTIES:
    :CUSTOM_ID: strassen
:END:

In 1969 **Strassen [cite:@strassen:1969]** published a simple 3 page paper that, for the first time, showed algorithms for matrix multiplication and inversion that were sub-$O(n^3)$. Though not fully recognized until later, this was a huge milestone for computing. While the matrix multiplication algorithm is an achievement with arguably greater impact we will focus on inversion. Strassen's matrix inversion operates by splitting the matrix into quadrants and running recursively on two of them. The algorithm is a prime example of a divide and conquer approach and is thus well suited for a parallel implementation.

However, there are some problems that prevent Strassen matrix inversion from being the ideal algorithm. Due to the recursive inversion only taking place on two of the quadrants, certain matrices can end up with recursive quadrants that are very close to singular, leading to high numerical error. A solution is to pivot the matrix at each level to whichever of the top-most quadrants has a higher determinant [cite:@bailey:1988], but this adds extra complexity to the algorithm. Another issue is that each level of recursion contains many intermittent matrix multiplications. All of these calculations must be stored, which more then doubles the space requirements of a typical inversion.

** Parallel Gauss-Jordan
:PROPERTIES:
    :CUSTOM_ID: sharma
:END:

While Gauss-Jordan is no longer the most efficient matrix inversion method, **Sharma et al [cite:@sharma:2013]** were able to redesign it into a highly parallel GPU based Gauss-Jordan algorithm. Their implementation showed impressive results; achieving $O(n)$ latency, though with some major caveats.

Sharma et al hits many of the limitations of GPU hardware. In order to avoid the extra I/O penalty associated with global GPU memory, the algorithm must be able to store one row and one column of the matrix in shared memory. Additionally, the GPU algorithm improves on the parallel $O(n^3)$ by doing $n^2$ of the work in parallel, thus if the GPU does not have at least $n^2$ parallel threads, than the latency will increase exponentially.

** In-Place Approach
:PROPERTIES:
    :CUSTOM_ID: xuebin
:END:

Typically, the Gauss-Jordan algorithm requires appending a $n \times n$ unit matrix to the original matrix. However, in 2013 **DasGupta [cite:@dasgupta:2013]** introduced a modified Gauss-Jordan algorithm that handles the inversion in-place. While this algorithm improves the space efficiency of Gauss-Jordan, it retains the time complexity of $O(n^3)$. In 2023, **Xuebin et al [cite:@xuebin:2023]** created a parallel modification of DasGupta's algorithm that is optimized for inverting many small matrices at a time on GPU. This algorithm is bound by similar limitations to the one in Sharma et al [cite:@sharma:2013] (see [[#sharma]]). Assuming $n \times \textit{number of matrices}$ is small enough to fit into the total number of parallel threads, then the algorithm runs in $O(n^2)$ time.

** GPU Thread-Data Remapping
:PROPERTIES:
    :CUSTOM_ID: cuneo
:END:

Due to the nature of GPU architecture, threads within the same warp are not able to execute different paths in parallel. This limits the performance of workloads that contain conditional branching or uneven allocation of work as branches are serialized. Largely divergent workloads can try to avoid this overhead by periodically reshuffling data to reduce the divergence inside warps; this technique is called Thread-Data Remapping (TDR). The most common form of TDR involves stopping all work at set intervals and performing synchronization. This approach is less than ideal since full workload synchronization requires the CPU to step in and handle workload discrepancies between runs. Communication between the CPU and GPU is expensive and should be avoided if possible.

A better approach, introduced by **Cuneo and Bailey [cite:@cuneo:2024]**, handles TDR entirely on-GPU by implementing a work scheduling mechanism that is reminiscent of the promise and future concurrency model. While not the first on-GPU TDR, Cuneo and Bailey's method is the first to allow remapping across blocks without synchronization.

* TODO Background

** Gauss-Jordan Elimination

In linear algebra we can utilize matrix multiplication to transform a matrix row-by-row. For instance the multiplication
#+NAME: swap
\begin{talign}
    \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
    \begin{bmatrix} a & b \\ c & d \end{bmatrix}
    & = \begin{bmatrix} c & d \\ a & b \end{bmatrix}
\end{talign}
swaps the rows of the right hand side matrix. Utilizing this technique we can define similar transformation matrices for scaling rows and adding multiples of one row to another (hence shifting a row by a multiple of another).

1. Swap one row with another (See [[swap]])
2. Scale a row
   \begin{talign}
       \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}
       \begin{bmatrix} a & b \\ c & d \end{bmatrix}
       & = \begin{bmatrix} 2a & 2b \\ c & d \end{bmatrix}
   \end{talign}
3. Shift a row by a multiple of another
   \begin{talign}
       \begin{bmatrix} 1 & 0 \\ 3 & 1 \end{bmatrix}
       \begin{bmatrix} a & b \\ c & d \end{bmatrix}
       & = \begin{bmatrix} a & b \\ c+3a & d+3b \end{bmatrix}
   \end{talign}

The process of Gauss-Jordan elimination utilizes these transformations to convert matrices to a canonical form where leading entries are 1 and 0s are present both above and below each leading entry. This form is called the reduced row-echelon form. If the matrix is fully reducible, then for a matrix with $n$ rows, the first $n$ columns form an identity matrix of size $n$. Thus, for an $n \times n$ matrix $M$, the given result of applying transformations $T_1$ to $T_i$ is the identity matrix $\symbf{I}_n$,
\begin{talign}
    T_n \dotsm T_2 T_1 M & = \symbf{I}_n
\end{talign}
Given that $M^{-1} M = \symbf{I}_n = M M^{-1}$ we can show that,
\begin{talign}
    T_n \dotsm T_2 T_1 M & = M^{-1} M \\
    T_n \dotsm T_2 T_1 M M^{-1} & = M^{-1} M M^{-1} \\
    T_n \dotsm T_2 T_1 \symbf{I}_n & = M^{-1}
\end{talign}
Therefore applying the same $T_1$ though $T_n$ operations to the identity matrix will result in the inverse of our matrix $M$. Utilizing this relationship we can invert a square matrix by performing Gauss-Jordan on the matrix $M|\symbf{I}$, $M$ concatenated with an identity matrix. The resulting matrix after Gauss-Jordan will be $\symbf{I}|M^{-1}$.

Rather than performing the full matrix multiplication for every Gauss-Jordan operation we can merely apply the arithmetic directly to the row, given that we represent the result of each transformation as an algebraic operation on a given row. For example, the transformation of doubling row 3 in a matrix can be written as $R_3 \gets 2 \times R_3$ and thus it is sufficient to multiply each element of row 3 by 2.

While the combination matrix of all transformation $T_1 T_2 \dotsm T_n$ is unique, the individual operations are not. For example
#+NAME: non-unique
\begin{talign}
    T_{R_1 \gets 2R_1} T_{\textit{swap}(R_1, R_2)}
    & = T_{\textit{swap}(R_1, R_2)} T_{R_2 \gets 2R_2}
\end{talign}
Thus, there are many methods of deriving a combination of operations. For this research we focus on the algorithm utilized by Sharma et al [cite:@sharma:2013] given in (Algorithm [[algo-1]]).

#+CAPTION: Gauss-Jordan Elimination
#+NAME: algo-1
\begin{algorithm*}[b]
    \KwIn{An augmented matrix $M$ that has $n$ rows}
    \ForEach{row $R_i$ in $M$}{
        \tcp{Step 1: Swap our current row for one with a non-zero $i\text{th}$ element.}
        Find $R_k$ where $R_{ki} \neq 0$

        $\text{swap}(R_i, R_k)$

        \tcp{Step 2: Divide our current row by its $i\text{th}$ element.}
        $R_i \gets R_i / R_{ii}$

        \tcp{Step 3: From every other row}
        \ForEach{row $R_j$ in $M$ where $j \neq i$}{
            \tcp{Step 3.1: subtract the $R_{ji}$ multiple of the $i\text{th}$ row.}
            $R_j \gets R_j - R_{ji} \times R_i$
        }
    }
\end{algorithm*}

*** Parallel Gauss-Jordan

The parallel Gauss-Jordan method, introduced by Sharma et al. [cite:@sharma:2013], leverages the algorithm outlined in (Algorithm [[algo-1]]) by executing each set of associative row operations concurrently. To determine the sets of operations that exhibit associativity and can thus be executed in parallel, we will assume everything is associative and disprove individual cases. The first evident disproof arises with the swap operation, which, as demonstrated in (Eqn [[non-unique]]), lacks associativity with other operations on the affected rows. Thus in our algorithm the swap, denoted /step 1/, must be taken in serial with its surrounding operations. This further causes the enclosing loop at line 1 to be serial; since each iteration introduces a new swap operation.

For our remaining operations, /step 2/ performs a scale on the $i\text{th}$ row, while the loop, /step 3/, performs a shift on every other row. Scaling is algebraically equivalent to scalar multiplication on a vector and thus scaling operations are associative with each other following the associativity of scalar multiplication. Shifting operations consist of a multiplication followed by an addition; since multiplication and addition together are not strictly associative it seems that shift operations are non-associative. However, it is also possible to perform operations in parallel if they are linearly independent. Each operation has a strict set of rows that it operates on so we can consider an operation linearly independent from operations on other rows. The shift operation technically operates on two rows but only transforms one of those rows. Thus shift operations can generally be considered linearly independent with other shift operations that utilize the same multiple row, but not with operations that modify the row. Therefore, /step 2/ and /step 3/ necessitate serial execution due to their shared operation on the row, whereas all /step 3.1/ actions can be carried out in parallel, given their linear independence.

So far we have been assuming row operations are atomic in that they perform operations on whole rows simultaneously. However, large enough matrices will necessitate that we split row operations into two or more step in order to process the entire row. This presents a problem in both /step 2/ and /step 3.1/ because we assume that the $i\text{th}$ element of the targeted row has not been modified during the operation. Therefore we must either ensure that the $i\text{th}$ element is modified last or that we store the $i\text{th}$ element elsewhere before performing the operation.

*** In-Place Gauss-Jordan

In the Gauss-Jordan method introduced above we operate on the matrix $M|\symbf{I}$ and utilize (Algorithm [[algo-1]]) to transform it to $\symbf{I}|M^{-1}$. However, the only resulting component of the matrix we care about is $M$. Further, after each iteration of the outer loop in our algorithm, half of our augmented matrix will be columns of the identity matrix. For example, given the matrix
\begin{talign}
    A = \begin{bNiceArray}{ccc|ccc}
        1 & 0 & 1 & 1 & 0 & 0 \\
        0 & 2 & 1 & 0 & 1 & 0 \\
        1 & 1 & 1 & 0 & 0 & 1
    \end{bNiceArray}
\end{talign}
after 2 iterations of the outer loop we will end up with
\begin{talign}
    A = \begin{bNiceArray}{ccc|ccc}
        1 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & \sfrac{1}{2} & 0 & \sfrac{1}{2} & 0 \\
        0 & 0 & -\sfrac{1}{2} & -1 & -\sfrac{1}{2} & 1
    \end{bNiceArray}
\end{talign}
The first 2 columns have already been inverted by the algorithm and the last column has been untouched, thus those columns put together form the identity matrix. Therefore, at every step we are wasting computational time and space processing columns of the identity matrix.

The in-place method introduced by DasGupta [cite:@dasgupta:2013] saves us this extra computation by storing the only the column of the inverse necessary to complete each iteration. To perform the in-place method we start with the matrix $M$ and perform the updated algorithm shown in (Algorithm [[algo-2]]). At the beginning of each iteration $i$, we add an additional /step 0/ where we store the $i\text{th}$ column of the matrix and replace it with the $i\text{th}$ column of the identity. In /step 1/ we additionally swap the stored $i$ and $k$ elements. Then, in /step 2/ we divide by the stored $i\text{th}$ element rather than its current value. And finally in /step 3.1/ we multiply by the $j\text{th}$ stored element rather than the $i\text{th}$ element of the $j\text{th}$ row.

#+CAPTION: In-Place Gauss-Jordan Elimination
#+NAME: algo-2
\begin{algorithm*}[b]
    \KwIn{An augmented matrix $M$ that has $n$ rows}
    \ForEach{row $R_i$ in $M$}{
        \tcp{Step 0: Store the $i\text{th}$ column in $C$.}
        \ForEach{row $R_m$ in $M$}{
            $C_m \gets R_{mi}$

            $R_{mi} \gets \symbf{I}_{mi}$
        }

        \tcp{Step 1: Swap our current row for one with a non-zero $i\text{th}$ element.}
        Find $R_k$ where $R_{ki} \neq 0$

        $\text{swap}(R_i, R_k)$

        $\text{swap}(C_i, C_k)$

        \tcp{Step 2: Divide our current row by its $i\text{th}$ element.}
        $R_i \gets R_i / C_i$

        \tcp{Step 3: From every other row}
        \ForEach{row $R_j$ in $M$ where $j \neq i$}{
            \tcp{Step 3.1: subtract the $R_{ji}$ multiple of the $i\text{th}$ row.}
            $R_j \gets R_j - C_j \times R_i$
        }
    }
\end{algorithm*}

** TODO SIMT Programming
:PROPERTIES:
    :CUSTOM_ID: simt
:END:

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

** TODO Thread-Data Remapping

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

* TODO Main Results

** TODO Deliverables

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

** TODO Performance Analysis

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

* TODO Conclusion

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Orci eu lobortis elementum nibh tellus molestie nunc non. Neque aliquam vestibulum morbi blandit cursus risus at ultrices. Aliquet bibendum enim facilisis gravida neque convallis a cras. Etiam erat velit scelerisque in dictum non. Sed turpis tincidunt id aliquet risus. Eu volutpat odio facilisis mauris. Duis at consectetur lorem donec massa sapien. Id aliquet risus feugiat in ante. Tincidunt augue interdum velit euismod in pellentesque massa placerat duis. Commodo nulla facilisi nullam vehicula ipsum a arcu. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Suspendisse ultrices gravida dictum fusce ut. Feugiat scelerisque varius morbi enim. Tincidunt arcu non sodales neque sodales ut.

* Bibliography :ignore:ignoreheading:

#+LATEX: \clearpage % Page break
#+LATEX: \onecolumn
#+LATEX: \setlength\bibitemsep{0.5\baselineskip}
#+LATEX: \nocite{*} % Use all citations
#+print_bibliography:
